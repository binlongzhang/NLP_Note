{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extensive-blackjack",
   "metadata": {},
   "source": [
    "# 通过torchtext获取数据\n",
    "\n",
    "数据说明\n",
    "\n",
    "- Train.csv共有由三列组成，分别为标签、新闻标题、新闻简述；其中标签用1，2，3，4表示依次对应classes中的内容；\n",
    "- test.csv与train.csv内容格式相同；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fourth-spanish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:04, 24515.08lines/s]\n",
      "120000lines [00:09, 13236.62lines/s]\n",
      "7600lines [00:00, 13982.22lines/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from torchtext.datasets import text_classification\n",
    "import os\n",
    "\n",
    "load_data_path = '../data/'\n",
    "\n",
    "if not os.path.isdir(load_data_path):\n",
    "    os.mkdir(load_data_path)\n",
    "    \n",
    "train_dataset,test_dataset = text_classification.DATASETS['AG_NEWS'](root=load_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-movie",
   "metadata": {},
   "source": [
    "# 构建带有embedding层的文本分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exotic-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,num_class):\n",
    "        super().__init__()\n",
    "        # 实例划embedding层，sparse=True表示每次对该层求解梯度只更新部分权重\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_dim,sparse=True)\n",
    "        # 实例化线性层，参数分别是embed_dim和num_calss\n",
    "        self.fc = nn.Linear(embed_dim,num_class)\n",
    "        # 为各层初始化权重\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"初始化权重函数\"\"\"\n",
    "        #指定初始权重的取值范围\n",
    "        initrange= 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange,initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange,initrange)\n",
    "        # 初始化偏置为0\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self,text):\n",
    "        \"\"\"\n",
    "        text：文本数值映射后的结果\n",
    "        return: 与类别尺寸相同的张量，用以判断文本类别\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # 将输入的text转化为符合batch_size大小数据\n",
    "        c = embedded.size(0)\n",
    "        # 不足批次的数据抛弃\n",
    "        embedded = embedded[:BATCH_SIZE*c]\n",
    "        #利用平均池化的方法求embedded中指定行数的列的平均值\n",
    "        #但平均池化是作用在行上并且要三位输入；\n",
    "        # 因此对新的embedded进行转置，并扩充维度\n",
    "        embedded = embedded.transpose(1,0).unsqueeze(0)\n",
    "        embedded = F.avg_pool1d(embedded,kernel_size = c)\n",
    "        \n",
    "        # 最后，减去新增的维度，转置回去输送给c\n",
    "        return self.fc(embedded[0].transpose(1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-heart",
   "metadata": {},
   "source": [
    "# 实例化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thirty-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词汇总数\n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "# 嵌入的维度\n",
    "EMBED_DIM = 32\n",
    "# 获取整个文本分类的总数\n",
    "NUM_CLASS = len(train_dataset.get_labels())\n",
    "\n",
    "# 实例化模型对象\n",
    "model = TextSentiment(VOCAB_SIZE,EMBED_DIM,NUM_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-murder",
   "metadata": {},
   "source": [
    "# 对数据进行batch处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entertaining-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    \"\"\"\n",
    "        生成batch数据\n",
    "        :param batch:由样本张量和对应标签元祖组成的batch_size大小的列表\n",
    "            如：[(sample1,label1),(sample2,label2),......,(sampleN,labelN)]\n",
    "        :return :样本张量和标签各自的列表\n",
    "            text = tensor([sample1,sample2,......,sampleN])\n",
    "            label = tensor([label1,label2,......,labelN])\n",
    "    \"\"\"\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    text = torch.cat(text)\n",
    "    return text,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rocky-wound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 3, 23,  2,  8,  3, 45, 21,  6]), tensor([1, 0]))\n"
     ]
    }
   ],
   "source": [
    "# 测试batch处理\n",
    "batch = [(1,torch.tensor([3,23,2,8])),(0,torch.tensor([3,45,21,6]))]\n",
    "res = generate_batch(batch)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-preparation",
   "metadata": {},
   "source": [
    "# 构建训练和验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subtle-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "def train(train_data):\n",
    "    # train_data: 代表传入的训练数据\n",
    "\n",
    "    # 初始化训练损失值和准确率\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    # 使用数据加载器构建批次数据\n",
    "    data = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "\n",
    "    # 对data进行循环遍历, 使用每个batch数据先进行训练\n",
    "    for i, (text, cls) in enumerate(data):\n",
    "        # 训练模型的第一步: 将优化器的梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 将一个批次的数据输入模型中, 进行预测\n",
    "        output = model(text.to(device))\n",
    "        # 用损失函数来计算预测值和真实标签之间的损失\n",
    "        loss = criterion(output.to(device), cls.to(device))\n",
    "        # 将该批次的损失值累加到总损失中\n",
    "        train_loss += loss.item()\n",
    "        # 进行反向传播的计算\n",
    "        loss.backward()\n",
    "        # 参数更新\n",
    "        optimizer.step()\n",
    "        # 计算该批次的准确率并加到总准确率上, 注意一点这里加的是准确的数字\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # 进行整个轮次的优化器学习率的调整\n",
    "    scheduler.step()\n",
    "\n",
    "    # 返回本轮次训练的平均损失值和平均准确率\n",
    "    return train_loss / len(train_data), train_acc / len(train_data)\n",
    "\n",
    "\n",
    "# 编写验证函数的代码\n",
    "def valid(valid_data):\n",
    "    # valid_data: 代表验证集的数据\n",
    "    # 初始化验证的损失值和准确率\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "\n",
    "    # 利用数据加载器构造每一个批次的验证数据\n",
    "    data = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    data\n",
    "    # 循环遍历验证数据\n",
    "    for text, cls in data:\n",
    "        # 注意: 在验证阶段, 一定要保证模型的参数不发生改变, 也就是不求梯度\n",
    "        with torch.no_grad():\n",
    "            # 将验证数据输入模型进行预测\n",
    "            output = model(text)\n",
    "            # 计算损失值\n",
    "            loss = criterion(output, cls)\n",
    "            # 将该批次的损失值累加到总损失值中\n",
    "            valid_loss += loss.item()\n",
    "            # 将该批次的准确数据累加到总准确数字中\n",
    "            valid_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # 返回本轮次验证的平均损失值和平均准确率\n",
    "    return valid_loss / len(valid_data), valid_acc / len(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-simple",
   "metadata": {},
   "source": [
    "# 模型的训练和验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "N_EPOCHS = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "# 定义损失函数、优化器\n",
    "min_valid_loss = float('inf')\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,1,gamma=0.9)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_,sub_valid_ = random_split(train_dataset,[train_len,len(train_dataset)-train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss,train_acc = train(sub_train_)\n",
    "    valid_loss,valid_acc = valid(sub_valid_)\n",
    "    \n",
    "    secs = int(time.time()-start_time)\n",
    "    \n",
    "    mins = secs/60\n",
    "    secs = secs%60\n",
    "    \n",
    "    print('Epoch: %d' % (epoch + 1), \" | time in %d minites, %d seconds\" % (mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-fitness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sentimentAnalysis]",
   "language": "python",
   "name": "conda-env-sentimentAnalysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
